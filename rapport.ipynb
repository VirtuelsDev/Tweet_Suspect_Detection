{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Rapport de Projet : Tweet Suspect Detection\n",
    "\n",
    "**Nom du Projet :** Tweet Suspect Detection  \n",
    "**Auteur :** Benewende Pierre Bonkoungou  \n",
    "**Date :** 13-11-2024\n",
    "\n",
    "____\n",
    "\n",
    "## 1. Contexte et Objectifs\n",
    "\n",
    "Les réseaux sociaux sont devenus des plateformes où les utilisateurs partagent divers types de contenu, mais cela inclut parfois des messages suspects ou nuisibles. **Tweet Suspect Detection** a pour objectif d’identifier de tels messages, afin de contribuer à la sécurité en ligne. En combinant des techniques de **Natural Language Processing (NLP)** et de **machine learning**, ce projet vise à développer une application capable de :\n",
    "\n",
    "- **Nettoyer et préparer** des données textuelles (tweets),\n",
    "- **Extraire des caractéristiques** pertinentes pour détecter les messages suspects,\n",
    "- **Entraîner un modèle** de classification,\n",
    "- **Déployer une interface** pour permettre aux utilisateurs de tester le modèle.\n",
    "\n",
    "## 2. Structure du Projet\n",
    "\n",
    "Le projet est organisé en plusieurs dossiers et fichiers pour faciliter l'exécution, la maintenance et le développement. La structure est décrite ci-dessous :\n",
    "\n",
    "```\n",
    "Tweet_Suspect_Detection/\n",
    "│\n",
    "├── data/\n",
    "│   ├── tweets_suspect.csv          # Données brutes (tweets non traités)\n",
    "│   ├── processed_data.csv    # Données après prétraitement (tweets nettoyés)\n",
    "│\n",
    "├── notebooks/\n",
    "│   ├── 1_exploration.ipynb   # Notebook pour l'analyse exploratoire des données\n",
    "│   ├── 2_preprocessing.ipynb # Notebook pour le nettoyage et le prétraitement des données\n",
    "│   ├── 3_model_training.ipynb     # Notebook pour l'entraînement des modèles\n",
    "│   └── 4_model_evaluation.ipynb   # Notebook pour l'évaluation et la validation des modèles\n",
    "│\n",
    "├── src/\n",
    "│   ├── __init__.py            # Fichier d'initialisation du package\n",
    "│   ├── data_preprocessing.py   # Script pour le prétraitement des données\n",
    "│   ├── feature_engineering.py  # Script pour la vectorisation et l'extraction des caractéristiques\n",
    "│   ├── model_training.py      # Script pour l'entraînement des modèles\n",
    "│   ├── model_evaluation.py    # Script pour l'évaluation et l'ajustement des modèles\n",
    "│   └── utils.py               # Script utilitaire (fonction de nettoyage, gestion des paramètres, etc.)\n",
    "│\n",
    "├── requirements.txt           # Liste des dépendances Python du projet\n",
    "├── app.py                     # Application Streamlit pour l'interface utilisateur\n",
    "├── deploy_model.py            # Script pour déployer le modèle en production\n",
    "├── README.md                  # Fichier de documentation du projet\n",
    "└── config.yaml                # Fichier de configuration (par exemple, pour les hyperparamètres)\n",
    "```\n",
    "\n",
    "### Détails des fichiers principaux\n",
    "\n",
    "1. **`data/`** : Contient les données brutes (`raw_data.csv`) et les données après nettoyage (`processed_data.csv`).\n",
    "   \n",
    "2. **`notebooks/`** : Inclut les notebooks pour chaque étape du projet, de l’exploration des données à l’évaluation des modèles.\n",
    "\n",
    "3. **`src/`** :\n",
    "   - **`data_preprocessing.py`** : Gère le nettoyage des tweets (suppression des URL, ponctuations, etc.).\n",
    "   - **`feature_engineering.py`** : Effectue la vectorisation des textes (TF-IDF, Word Embeddings).\n",
    "   - **`model_training.py`** : Contient le code pour entraîner les modèles (Logistic Regression, Random Forest, etc.).\n",
    "   - **`model_evaluation.py`** : Évalue les modèles à l'aide de métriques de performance.\n",
    "   - **`utils.py`** : Fonctions auxiliaires pour le chargement de données et le nettoyage.\n",
    "\n",
    "4. **`app.py`** : Interface utilisateur développée avec Streamlit.\n",
    "\n",
    "5. **`deploy_model.py`** : Gère le déploiement du modèle en production.\n",
    "\n",
    "6. **`config.yaml`** : Stocke les hyperparamètres et autres configurations du projet.\n",
    "\n",
    "## 3. Description des Étapes de Développement\n",
    "\n",
    "### 3.1 Analyse Exploratoire des Données (`1_data_exploration.ipynb`)\n",
    "   - **Objectif** : Comprendre la structure et le contenu des données brutes.\n",
    "   - **Actions réalisées** :\n",
    "     - Chargement des données depuis `data/raw_data.csv`.\n",
    "     - Analyse de la distribution des mots, fréquences des termes, et visualisation des mots les plus fréquents pour comprendre les schémas sous-jacents.\n",
    "   - **Résultats** : Identification des mots communs et des structures de langage, qui ont orienté le choix des techniques de prétraitement.\n",
    "\n",
    "### 3.2 Prétraitement des Données (`2_data_preprocessing.ipynb` et `data_preprocessing.py`)\n",
    "   - **Objectif** : Nettoyer les données pour les rendre exploitables par les modèles.\n",
    "   - **Étapes** :\n",
    "     - **Suppression des URLs, mentions, ponctuation** : Nettoyage du texte pour enlever les éléments non pertinents.\n",
    "     - **Tokenisation et suppression des stop words** : Découpage des phrases en mots pour analyse et suppression des mots les moins informatifs.\n",
    "   - **Sortie** : Enregistrement des tweets nettoyés dans `processed_data.csv`.\n",
    "\n",
    "### 3.3 Extraction des Caractéristiques (`feature_engineering.py`)\n",
    "   - **Objectif** : Convertir le texte en vecteurs numériques pour l'analyse.\n",
    "   - **Techniques** :\n",
    "     - **TF-IDF (Term Frequency-Inverse Document Frequency)** : Utilisé pour capturer l'importance de mots en fonction de leur fréquence.\n",
    "     - **Word Embeddings** : Représentations vectorielles des mots pour capturer les relations sémantiques entre termes.\n",
    "   - **Impact** : Améliore la capacité des modèles à distinguer les tweets suspects des autres.\n",
    "\n",
    "### 3.4 Entraînement des Modèles (`3_model_training.ipynb` et `model_training.py`)\n",
    "   - **Objectif** : Construire un modèle de classification efficace pour détecter les tweets suspects.\n",
    "   - **Modèles testés** :\n",
    "     - **Logistic Regression** : Modèle de référence pour la classification binaire.\n",
    "     - **Random Forest** : Apprentissage basé sur les arbres, utilisé pour capturer les non-linéarités.\n",
    "     - **SVM (Support Vector Machine)** : Pour maximiser la marge de séparation entre classes.\n",
    "   - **Entraînement** : Chaque modèle est entraîné sur les données transformées.\n",
    "   - **Optimisation** : Recherche des hyperparamètres optimaux pour chaque modèle en utilisant `config.yaml`.\n",
    "\n",
    "### 3.5 Évaluation et Validation des Modèles (`4_model_evaluation.ipynb` et `model_evaluation.py`)\n",
    "   - **Objectif** : Mesurer la performance des modèles sur un ensemble de test.\n",
    "   - **Métriques** :\n",
    "     - **Accuracy** : Taux de précision global.\n",
    "     - **Recall et F1-Score** : Mesure de la capacité à identifier les tweets suspects.\n",
    "     - **Courbe ROC et AUC** : Indicateurs pour évaluer la performance du modèle dans la classification binaire.\n",
    "   - **Résultat** : Le modèle **[Nom du modèle sélectionné]** a montré la meilleure performance et a été choisi pour la phase de production.\n",
    "\n",
    "### 3.6 Déploiement et Interface Utilisateur (`app.py` et `deploy_model.py`)\n",
    "   - **Objectif** : Permettre aux utilisateurs d'accéder au modèle via une interface interactive.\n",
    "   - **Technologie utilisée** : Streamlit, un framework Python pour les applications web interactives.\n",
    "   - **Fonctionnalité** :\n",
    "     - L’utilisateur peut entrer un tweet dans l’interface Streamlit pour voir s'il est détecté comme suspect.\n",
    "   - **Déploiement** : `deploy_model.py` contient le script de déploiement pour rendre le modèle accessible en ligne.\n",
    "\n",
    "## 4. Exécution du Projet\n",
    "\n",
    "### Prérequis\n",
    "Installez les dépendances listées dans `requirements.txt` en exécutant la commande :\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Instructions d'Exécution\n",
    "\n",
    "1. **Analyse et Prétraitement des Données** :\n",
    "   - Lancez les notebooks dans `notebooks/` pour explorer et préparer les données, dans cet ordre :\n",
    "     - `1_data_exploration.ipynb`\n",
    "     - `2_data_preprocessing.ipynb`\n",
    "\n",
    "2. **Entraînement et Évaluation des Modèles** :\n",
    "   - Utilisez `3_model_training.ipynb` pour entraîner les modèles et `4_model_evaluation.ipynb` pour évaluer leurs performances.\n",
    "\n",
    "3. **Interface Utilisateur** :\n",
    "   - Lancer l'application Streamlit avec la commande suivante :\n",
    "     ```bash\n",
    "     streamlit run app.py\n",
    "     ```\n",
    "   - Accédez à l'interface pour tester le modèle avec de nouveaux tweets.\n",
    "\n",
    "## 5. Conclusion et Perspectives\n",
    "\n",
    "Le projet **Tweet Suspect Detection** constitue une solution efficace pour l'identification de tweets suspects, exploitant des techniques avancées de traitement du langage naturel. Le modèle final peut être déployé pour une utilisation en production, et l'interface utilisateur permet un usage intuitif et rapide.\n",
    "\n",
    "### Améliorations Futures\n",
    "- **Support Multilingue** : Ajout de données dans d'autres langues pour augmenter la robustesse.\n",
    "- **Utilisation de Deep Learning** : Essai de modèles avancés comme LSTM ou BERT pour améliorer les performances.\n",
    "- **Analyse en Temps Réel** : Mise en place d'un système pour analyser les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imbalanced-learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
