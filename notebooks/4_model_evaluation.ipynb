{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation du modèle**\n",
    "Dans ce notebook `4_model_evaluation.ipynb`, nous allons évaluer les modèles entraînés pour obtenir une vue détaillée de leurs performances sur des données de test. \n",
    "\n",
    "Cela inclura des analyses de métriques de classification, des courbes ROC-AUC, et des rapports sur la précision, le rappel et le F1-score pour chaque modèle. \n",
    "\n",
    "Ce notebook permet de comprendre comment les modèles se comportent et où des ajustements peuvent être nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importer les fonctions nécessaires depuis le package src\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data, load_pickle, evaluate_model, plot_confusion_matrix, plot_roc_curve\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Charger les données de test\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df_test \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/processed_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Assurez-vous d'avoir un ensemble de test séparé ou utilisez les mêmes données\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "# Importer les fonctions nécessaires depuis le package src\n",
    "from src import load_data, load_pickle, evaluate_model, plot_confusion_matrix, plot_roc_curve\n",
    "\n",
    "# Charger les données de test\n",
    "df_test = load_data('../data/processed_data.csv')  # Assurez-vous d'avoir un ensemble de test séparé ou utilisez les mêmes données\n",
    "\n",
    "# Vectoriser les tweets de test en utilisant le vectoriseur sauvegardé\n",
    "from src.feature_engineering import load_vectorizer\n",
    "vectorizer = load_vectorizer('../models/tfidf_vectorizer.pkl')\n",
    "X_test = vectorizer.transform(df_test['cleaned_tweet'])\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Charger le modèle entraîné\n",
    "model = load_pickle('../models/svm_model.pkl')  # Remplacez par le modèle souhaité\n",
    "\n",
    "# Évaluer le modèle\n",
    "metrics = evaluate_model(model, X_test, y_test)\n",
    "print(\"Rapport de classification :\")\n",
    "print(metrics['classification_report'])\n",
    "print(\"Précision du modèle :\", metrics['accuracy'])\n",
    "\n",
    "# Afficher la matrice de confusion\n",
    "conf_matrix = metrics['confusion_matrix']\n",
    "classes = ['Non Suspect', 'Suspect']\n",
    "plot_confusion_matrix(conf_matrix, classes)\n",
    "\n",
    "# Tracer la courbe ROC (pour classification binaire)\n",
    "plot_roc_curve(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importations nécessaires\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle et le vectoriseur sauvegardés\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "best_model = joblib.load('best_svm_model.pkl')  # On peut remplacer par le modèle souhaité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données de test (assurez-vous qu'elles sont identiques à celles utilisées dans l'entraînement)\n",
    "df_test = pd.read_pickle('df_data_cleaned.pkl')\n",
    "\n",
    "# Préparation des données de test\n",
    "X_test = tfidf_vectorizer.transform(df_test['cleaned_tweet'])\n",
    "y_test = df_test['label']\n",
    "\n",
    "# Prédictions sur les données de test\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation des performances avec des métriques\n",
    "print(\"Rapport de classification :\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Matrice de confusion :\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la précision\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Précision du modèle: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la matrice de confusion\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Matrice de Confusion\")\n",
    "plt.xlabel(\"Prédiction\")\n",
    "plt.ylabel(\"Réel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation de la courbe ROC-AUC (pour les modèles binaires)\n",
    "if len(set(y_test)) == 2:  # Vérifier si c'est une classification binaire\n",
    "    y_test_binarized = label_binarize(y_test, classes=[0, 1])\n",
    "    y_score = best_model.decision_function(X_test)  # Pour SVM ou Logistic Regression\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_binarized, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Tracer la courbe ROC\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"Courbe ROC (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"Taux de Faux Positifs (FPR)\")\n",
    "    plt.ylabel(\"Taux de Vrais Positifs (TPR)\")\n",
    "    plt.title(\"Courbe ROC\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"La courbe ROC-AUC est uniquement disponible pour la classification binaire.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyse des performances de chaque classe (optionnel pour des classes multiples)\n",
    "if len(set(y_test)) > 2:\n",
    "    for label in set(y_test):\n",
    "        print(f\"\\nClasse {label} - Précision : {precision_score(y_test, y_pred, pos_label=label):.2f}\")\n",
    "        print(f\"Classe {label} - Rappel : {recall_score(y_test, y_pred, pos_label=label):.2f}\")\n",
    "        print(f\"Classe {label} - F1-Score : {f1_score(y_test, y_pred, pos_label=label):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explication des étapes du notebook**\n",
    "\n",
    "1. **Chargement du modèle et du vectoriseur** :\n",
    "   - Le modèle pré-entraîné et le vectoriseur TF-IDF sont chargés avec `joblib`. Assurez-vous de charger le bon modèle que vous souhaitez évaluer.\n",
    "\n",
    "2. **Préparation des données de test** :\n",
    "   - Nous chargeons les données de test et appliquons la transformation TF-IDF avec le vectoriseur sauvegardé. La variable `X_test` contient les caractéristiques du texte transformées, et `y_test` les étiquettes réelles.\n",
    "\n",
    "3. **Prédictions et Rapport de Classification** :\n",
    "   - Les prédictions sur les données de test sont obtenues avec `predict`.\n",
    "   - Nous utilisons `classification_report` pour obtenir des métriques détaillées (précision, rappel, F1-score) pour chaque classe.\n",
    "   - La matrice de confusion est affichée avec `confusion_matrix`, permettant d'analyser où le modèle a fait des erreurs de classification.\n",
    "\n",
    "4. **Calcul et Affichage de la Précision** :\n",
    "   - La précision globale du modèle est calculée avec `accuracy_score`, une mesure de performance générale indiquant la proportion des prédictions correctes.\n",
    "\n",
    "5. **Visualisation de la Matrice de Confusion** :\n",
    "   - La matrice de confusion est visualisée sous forme de carte thermique avec `seaborn` pour une analyse visuelle rapide.\n",
    "\n",
    "6. **Courbe ROC-AUC** (Classification Binaire uniquement) :\n",
    "   - Si les données de test comportent seulement deux classes, nous calculons et affichons la courbe ROC et l'AUC.\n",
    "   - La courbe ROC illustre le compromis entre le taux de faux positifs et de vrais positifs. Une AUC proche de 1 indique une bonne performance.\n",
    "\n",
    "7. **Analyse par Classe** (Classification Multiclasse) :\n",
    "   - Pour chaque classe, la précision, le rappel et le F1-score sont calculés et affichés. Cela peut être particulièrement utile pour des données déséquilibrées ou lorsque certaines classes sont plus importantes.\n",
    "\n",
    "### **Métriques et Visualisations pour l'évaluation**\n",
    "\n",
    "- **Rapport de Classification** : Donne des informations par classe avec précision, rappel et F1-score.\n",
    "- **Matrice de Confusion** : Montre les erreurs spécifiques faites par le modèle, indiquant quelles classes sont confondues.\n",
    "- **Précision** : Fournit un aperçu général de l'exactitude du modèle.\n",
    "- **Courbe ROC-AUC** : Utile pour évaluer les performances sur une tâche de classification binaire, en montrant le compromis entre le rappel et la spécificité.\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "Le notebook `4_model_evaluation.ipynb` fournit une analyse complète des performances des modèles, avec des visualisations et des métriques permettant de bien comprendre les capacités de généralisation. Ces étapes permettent d’identifier les améliorations possibles et d’affiner le modèle pour de meilleures performances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
